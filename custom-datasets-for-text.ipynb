{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os  # when loading file paths\nimport pandas as pd  # for lookup in annotation file\nimport spacy  # for tokenizer\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence  # pad batch\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image  # Load img\nimport torchvision.transforms as transforms\n\n\n# We want to convert text -> numerical values\n# 1. We need a Vocabulary mapping each word to a index\n# 2. We need to setup a Pytorch dataset to load the data\n# 3. Setup padding of every batch (all examples should be\n#    of same seq_len and setup dataloader)\n# Note that loading the image is very easy compared to the text!\n\n# Download with: python -m spacy download en\nspacy_eng = spacy.load(\"en\")\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n\n        # Get img, caption columns\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n\n        # Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n\n        return imgs, targets","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_loader(\n    root_folder,\n    annotation_file,\n    transform,\n    batch_size=32,\n    num_workers=8,\n    shuffle=True,\n    pin_memory=True,\n):\n    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n    )\n\n    return loader, dataset\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    transform = transforms.Compose(\n        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n    )\n\n    loader, dataset = get_loader(\n        \"../input/flickr8k/Images\", \"../input/flickr8k/captions.txt\", transform=transform\n    )\n\n    for idx, (imgs, captions) in enumerate(loader):\n        print(imgs.shape)\n        print(captions.shape)\n        ","execution_count":26,"outputs":[{"output_type":"stream","text":"torch.Size([32, 3, 224, 224])\ntorch.Size([23, 32])\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}